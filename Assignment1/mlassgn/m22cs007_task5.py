# -*- coding: utf-8 -*-
"""M22CS007_task5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lVhbPHhCZL2zqWibYNQp4eWKUalJ3qEV
"""

##5.1 sigmoid function

# Import matplotlib, numpy and math
import matplotlib.pyplot as plt
import numpy as np
import math

x = np.linspace(-100, 100, 200)
z = 1/(1 + np.exp(-x))

plt.plot(x, z)
plt.xlabel("x")
plt.ylabel("Sigmoid(X)")

plt.show()







#5.2 building a linear classifier

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.datasets import make_classification


from sklearn import preprocessing

plt.rc("font", size=14)

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['Algerian_forest_fires_dataset1.csv']))
df

df.describe()

df.size

df.shape

list(df.columns)

df.head()

#splitting the data into x and y
y = df['output']
df.drop("output",axis=1,inplace=True)
y

df.info()

print(df.isnull().sum())

from sklearn.model_selection import train_test_split
X_train, X_test,y_train,y_test = train_test_split(df,y,test_size=0.4)
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train,y_train)

pred = lr.predict(X_test)

import sklearn.metrics as metrics
print(metrics.accuracy_score(pred,y_test))







## 5-3, 60:40, using gradient descent

from sklearn.model_selection import train_test_split
X_train, X_test,y_train,y_test = train_test_split(df,y,test_size=0.4)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train,y_train)

# cost function
def cost(x_data, params,y_data):
  total_cost=0
  for i in range(x_data.shape[0]):
    total_cost+=(1/x_data.shape[0])*((x_data[i]*params).sum()-y_data[i])**2
  return total_cost
  
#gardient descent
def gd(x_data,y_data,params,lr,iter_value):
  

  for i in range(iter_value):
    slopes=np.zeros(x_data.shape[1])
    for j in range(x_data.shape[0]):
      for k in range(x_data.shape[1]):
        slopes[k]  += (1/x_data.shape[0])*((x_data[j]*params).sum()-y_data[j])*x_data[j][k]
    params=params-lr*slopes
    # print(cost(x_data,params,y_data))
  
  return params

def sigmoid(x):
  z=1/(1+np.exp(-x))
  return z

def predict(x_train,params):
  linear_model=np.dot(x_train,params)
  y_pred=sigmoid(linear_model)
  y_pred_clss=[1 if i > 0.5 else 0 for i in y_pred]
  return y_pred_clss

def logloss(y,y_pred):
  logerror=0
  logerrormean=0
  for i in range(y.shape[0]):
    logerrormean+=(1/y.shape[0])*abs(y[i]-y_pred[i])
    logerror+=(y[i]-y_pred[i])**2
  return logerror,logerrormean

params=np.zeros(x.shape[0]+1)
# y_pred=np.zeros(x_train.shape[1])

lr=0.00001
iter_val=500
params=gd(X_train,y_train,params,lr,iter_val)
print(params)
predict_y_train=predict(X_test,params)

print(predict_y_train)
print("loss in linear reg training =",cost(x_train,params,y_train))
print("loss in linear reg testing =",cost(x_test,params,y_test))

logerror,logerrormean=logloss(y_test,predict_y_train)
print("logrithimerrormean=",logerrormean)







###5.4 AUC-ROC curve

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, pred)
print(confusion_matrix)

