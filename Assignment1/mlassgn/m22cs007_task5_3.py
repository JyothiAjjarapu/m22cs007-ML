# -*- coding: utf-8 -*-
"""M22CS007_task5.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J3oE7gy70RrT9NzqFx7lu1xRaXxXHHNi
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
import pandas as pd

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['glass.csv']))
df

columnnames=["Id_number","RI","Na","Mg","Al","Si",
            "K","Ca","Ba","Fe","window_glass"]
df= pd.read_csv("glass.csv", header=None, names=columnnames )

df.head()

newdf=df.replace({'window_glass' : { 7 : 0, 6 : 0, 5:0}})
newdf=df1.replace({'window_glass' : { 1 : 1, 2 : 1, 3:1}})
x=df.drop(['window_glass'],axis=1).values
y=newdf['window_glass'].values
newdf
y.shape[0]

data1 = newdf.astype('float')


shuffle_df = data1.sample(frac=1)

train_size = int(0.6 * len(data1))

# Split your dataset 
t_set = shuffle_df[:train_size]
v_set = shuffle_df[train_size:]

x_t=t_set.drop(['window_glass'],axis=1)
x_t[x_t.shape[1]]=1
y_t=t_set['window_glass']

x_t=np.array(x_t)
y_t=np.array(y_t)


x_v=v_set.drop(['window_glass'],axis=1)
x_v[x_v.shape[1]]=1
y_v=v_set['window_glass']

x_v=np.array(x_v)
y_v=np.array(y_v)



y_t.shape[0]



"""**Logistic Regression using Gardient descent function**"""

# cost function
def cost(x_data, params,y_data):
  total_cost=0
  for i in range(x_data.shape[0]):
    total_cost+=(1/x_data.shape[0])*((x_data[i]*params).sum()-y_data[i])**2
  return total_cost
  
#gardient descent
def gd(x_data,y_data,params,lr,iter_value):
  

  for i in range(iter_value):
    slopes=np.zeros(x_data.shape[1])
    for j in range(x_data.shape[0]):
      for k in range(x_data.shape[1]):
        slopes[k]  += (1/x_data.shape[0])*((x_data[j]*params).sum()-y_data[j])*x_data[j][k]
    params=params-lr*slopes
    # print(cost(x_data,params,y_data))
  
  return params

def sigmoid(x):
  z=1/(1+np.exp(-x))
  return z

def predict(x_train,params):
  linear_model=np.dot(x_train,params)
  y_pred=sigmoid(linear_model)
  y_pred_clss=[1 if i > 0.6 else 0 for i in y_pred]
  return y_pred_clss

def logloss(y,y_pred):
  logerror=0
  logerrormean=0
  for i in range(y.shape[0]):
    logerrormean+=(1/y.shape[0])*abs(y[i]-y_pred[i])
    logerror+=(y[i]-y_pred[i])**2
  return logerror,logerrormean

params=np.zeros(x.shape[1]+1)
# y_pred=np.zeros(x_train.shape[1])

lr=0.00001
iter_val=200
params=gd(x_t,y_t,params,lr,iter_val)
print(params)
predict_y_t=predict(x_v,params)

print(predict_y_t)
# print("loss in linear reg training =",cost(x_train,params,y_train))
# print("loss in linear reg testing =",cost(x_test,params,y_test))

logerror,logerrormean=logloss(y_v,predict_y_t)
print("logrithimerrormean=",logerrormean)

